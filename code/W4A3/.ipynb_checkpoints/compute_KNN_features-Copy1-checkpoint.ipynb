{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will need to implement features, based on nearest neighbours. \n",
    "\n",
    "KNN classifier (regressor) is a very powerful model, when the features are homogeneous and it is a very common practice to use KNN as first level model. In this homework we will extend KNN model and compute more features, based on nearest neighbors and their distances. \n",
    "\n",
    "You will need to implement a number of features, that were one of the keys, that leaded the instructors to prizes in [Otto](https://www.kaggle.com/c/otto-group-product-classification-challenge) and [Springleaf](https://www.kaggle.com/c/springleaf-marketing-response) competitions. Of course, the list of features you will need to implement can be extended, in fact in competitions the list was at list 3 times larger, so when solving a real competition do not hesitate to make up your own features.   \n",
    "\n",
    "You can optionally implement parallel computations. Nearest neighbours are hard to compute so it is preferable to have a parallel version of all algorithms. Is is really a cool skill to have to know how to use multiprocessing, so you can try it here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.1\n",
      "pandas 0.20.3\n",
      "sklearn 0.19.0\n",
      "scipy 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "\n",
    "for p in [np, pd, sklearn, scipy]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The versions should be not less than:\n",
    "\n",
    "    numpy 1.13.1\n",
    "    pandas 0.20.3\n",
    "    sklearn 0.19.0\n",
    "    scipy 0.19.1\n",
    "    \n",
    "If it is not the case, uncomment the following cell and run it. Then restart the kernel and check versions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ..........^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\", line 384, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 256, in recv_into\n",
      "    return self.connection.recv_into(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/OpenSSL/SSL.py\", line 1335, in recv_into\n",
      "    self._raise_ssl_error(self._ssl, result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/OpenSSL/SSL.py\", line 1149, in _raise_ssl_error\n",
      "    raise WantReadError()\n",
      "OpenSSL.SSL.WantReadError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/conda\", line 6, in <module>\n",
      "    sys.exit(conda.cli.main())\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/cli/main.py\", line 164, in main\n",
      "    return conda_exception_handler(_main, *args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/exceptions.py\", line 573, in conda_exception_handler\n",
      "    return_value = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/cli/main.py\", line 134, in _main\n",
      "    exit_code = args.func(args, p)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/cli/main_update.py\", line 65, in execute\n",
      "    install(args, parser, 'update')\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/cli/install.py\", line 222, in install\n",
      "    unknown=index_args['unknown'], prefix=prefix)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/core/index.py\", line 92, in get_index\n",
      "    index = fetch_index(channel_priority_map, use_cache=use_cache)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/core/index.py\", line 111, in fetch_index\n",
      "    repodatas = collect_all_repodata(use_cache, tasks)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/core/repodata.py\", line 74, in collect_all_repodata\n",
      "    repodatas = _collect_repodatas_serial(use_cache, tasks)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/core/repodata.py\", line 463, in _collect_repodatas_serial\n",
      "    for url, schan, pri in tasks]\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/core/repodata.py\", line 463, in <listcomp>\n",
      "    for url, schan, pri in tasks]\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/core/repodata.py\", line 109, in func\n",
      "    res = f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/core/repodata.py\", line 442, in fetch_repodata\n",
      "    mod_etag_headers.get('_mod'))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/core/repodata.py\", line 138, in fetch_repodata_remote_request\n",
      "    timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/sessions.py\", line 501, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/sessions.py\", line 488, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/sessions.py\", line 609, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/adapters.py\", line 423, in send\n",
      "    timeout=timeout\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\", line 594, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\", line 387, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/conda/lib/python3.6/http/client.py\", line 1331, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/conda/lib/python3.6/http/client.py\", line 297, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/conda/lib/python3.6/http/client.py\", line 258, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/conda/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 269, in recv_into\n",
      "    [self.socket], [], [], self.socket.gettimeout())\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!conda update -y scikit-learn pandas scipy numpy joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/conda\", line 6, in <module>\r\n",
      "    sys.exit(conda.cli.main())\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/cli/main.py\", line 163, in main\r\n",
      "    from ..exceptions import conda_exception_handler\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/exceptions.py\", line 16, in <module>\r\n",
      "    from .common.url import maybe_unquote\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/common/url.py\", line 10, in <module>\r\n",
      "    from .path import split_filename\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/conda/common/path.py\", line 23, in <module>\r\n",
      "    from cytoolz.itertoolz import accumulate, concat, take\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/cytoolz/__init__.py\", line 1, in <module>\r\n",
      "    from .itertoolz import *\r\n",
      "  File \"cytoolz/itertoolz.pyx\", line 19, in init cytoolz.itertoolz (cytoolz/itertoolz.c:29827)\r\n",
      "  File \"cytoolz/utils.pyx\", line 10, in init cytoolz.utils (cytoolz/utils.c:1765)\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/toolz/__init__.py\", line 3, in <module>\r\n",
      "    from .functoolz import *\r\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/toolz/functoolz.py\", line 2, in <module>\r\n",
      "    import inspect\r\n",
      "  File \"/opt/conda/lib/python3.6/inspect.py\", line 1393, in <module>\r\n",
      "    Traceback = namedtuple('Traceback', 'filename lineno function code_context index')\r\n",
      "  File \"/opt/conda/lib/python3.6/collections/__init__.py\", line 428, in namedtuple\r\n",
      "    exec(class_definition, namespace)\r\n",
      "  File \"<string>\", line 1, in <module>\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!conda install -y joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!conda clean --index-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn features and labels. These features are actually OOF predictions of linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../readonly/KNN_features_data/X.npz'\n",
    "train_labels = '../readonly/KNN_features_data/Y.npy'\n",
    "\n",
    "test_path = '../readonly/KNN_features_data/X_test.npz'\n",
    "test_labels = '../readonly/KNN_features_data/Y_test.npy'\n",
    "\n",
    "# Train data\n",
    "X = scipy.sparse.load_npz(train_path)\n",
    "Y = np.load(train_labels)\n",
    "\n",
    "# Test data\n",
    "X_test = scipy.sparse.load_npz(test_path)\n",
    "Y_test = np.load(test_labels)\n",
    "\n",
    "# Out-of-fold features we loaded above were generated with n_splits=4 and skf seed 123\n",
    "# So it is better to use seed 123 for generating KNN features as well \n",
    "skf_fold = 123\n",
    "n_splits = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you need to implement features, based on nearest neaighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NearestNeighborsFeats(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "        This class should implement KNN features extraction \n",
    "    '''\n",
    "    def __init__(self, n_jobs, k_list, metric, n_classes=None, n_neighbors=None, eps=1e-6):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.k_list = k_list\n",
    "        self.metric = metric\n",
    "        \n",
    "        if n_neighbors is None:\n",
    "            self.n_neighbors = max(k_list) \n",
    "        else:\n",
    "            self.n_neighbors = n_neighbors\n",
    "            \n",
    "        self.eps = eps        \n",
    "        self.n_classes_ = n_classes\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "            Set's up the train set and self.NN object\n",
    "        '''\n",
    "        # Create a NearestNeighbors (NN) object. We will use it in `predict` function \n",
    "        self.NN = NearestNeighbors(n_neighbors=max(self.k_list), \n",
    "                                      metric=self.metric, \n",
    "                                      n_jobs=1, \n",
    "                                      algorithm='brute' if self.metric=='cosine' else 'auto')\n",
    "        self.NN.fit(X)\n",
    "        \n",
    "        # Store labels \n",
    "        self.y_train = y\n",
    "        \n",
    "        # Save how many classes we have\n",
    "        self.n_classes = np.unique(y).shape[0] if self.n_classes_ is None else self.n_classes_\n",
    "        \n",
    "        \n",
    "    def predict(self, X):       \n",
    "        '''\n",
    "            Produces KNN features for every object of a dataset X\n",
    "        '''\n",
    "        if self.n_jobs == 1:\n",
    "            test_feats = []\n",
    "            for i in range(X.shape[0]):\n",
    "                test_feats.append(self.get_features_for_one(X[i:i+1]))\n",
    "        else:\n",
    "            '''\n",
    "                 *Make it parallel*\n",
    "                     Number of threads should be controlled by `self.n_jobs`  \n",
    "                     \n",
    "                     \n",
    "                     You can use whatever you want to do it\n",
    "                     For Python 3 the simplest option would be to use \n",
    "                     `multiprocessing.Pool` (but don't use `multiprocessing.dummy.Pool` here)\n",
    "                     You may try use `joblib` but you will most likely encounter an error, \n",
    "                     that you will need to google up (and eventually it will work slowly)\n",
    "                     \n",
    "                     For Python 2 I also suggest to use `multiprocessing.Pool` \n",
    "                     You will need a hint from this blog \n",
    "                     http://qingkaikong.blogspot.ru/2016/12/python-parallel-method-in-class.html\n",
    "                     I could not get `joblib` working at all for this code \n",
    "                     (but in general `joblib` is very convenient)\n",
    "                     \n",
    "            '''\n",
    "            pool = Pool(processes=self.n_jobs)\n",
    "            test_feats = pool.map(self.get_features_for_one, [X[i:i+1] for i in range(X.shape[0])])\n",
    "\n",
    "#             assert False, 'You need to immplement it for n_jobs > 1'\n",
    "            \n",
    "            # YOUR CODE GOES HERE\n",
    "            # test_feats =  # YOUR CODE GOES HERE\n",
    "            # YOUR CODE GOES HERE\n",
    "            \n",
    "        return np.vstack(test_feats)\n",
    "        \n",
    "        \n",
    "    def get_features_for_one(self, x):\n",
    "        '''\n",
    "            Computes KNN features for a single object `x`\n",
    "        '''\n",
    "\n",
    "        NN_output = self.NN.kneighbors(x)\n",
    "        \n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores indices of the neighbors\n",
    "        neighs = NN_output[1][0]\n",
    "        \n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores distances to corresponding neighbors\n",
    "        neighs_dist = NN_output[0][0] \n",
    "\n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores labels of corresponding neighbors\n",
    "        neighs_y = self.y_train[neighs] \n",
    "        \n",
    "        ## ========================================== ##\n",
    "        ##              YOUR CODE BELOW\n",
    "        ## ========================================== ##\n",
    "        \n",
    "        # We will accumulate the computed features here\n",
    "        # Eventually it will be a list of lists or np.arrays\n",
    "        # and we will use np.hstack to concatenate those\n",
    "        return_list = [] \n",
    "        \n",
    "        \n",
    "        ''' \n",
    "            1. Fraction of objects of every class.\n",
    "               It is basically a KNNСlassifiers predictions.\n",
    "\n",
    "               Take a look at `np.bincount` function, it can be very helpful\n",
    "               Note that the values should sum up to one\n",
    "        '''\n",
    "        for k in self.k_list:\n",
    "            feats = np.bincount(neighs_y[:k], minlength=self.n_classes)/float(k)\n",
    "            \n",
    "            assert len(feats) == self.n_classes\n",
    "            return_list += [feats]\n",
    "        \n",
    "        \n",
    "        '''\n",
    "            2. Same label streak: the largest number N, \n",
    "               such that N nearest neighbors have the same label\n",
    "               \n",
    "               What can help you:  np.where, list.index(), map\n",
    "        '''\n",
    "        wh = np.where(neighs_y != neighs_y[0])[0]\n",
    "        if len(wh) == 0:\n",
    "            feats = [len(neighs_y) + 1]\n",
    "        else:\n",
    "            feats = [np.min(wh)]\n",
    "            \n",
    "#         except :\n",
    "#             feats = [len(neighs_y) + 1]\n",
    "     \n",
    "        assert len(feats) == 1\n",
    "        return_list += [feats]\n",
    "        \n",
    "        '''\n",
    "            3. Minimum distance to objects of each class\n",
    "               Find first instance of a class and take it's distance as features.\n",
    "               \n",
    "               If there are no neighboring objects of some classes, \n",
    "               Then set distance to that class to be 999\n",
    "\n",
    "               `np.where` might be helpful\n",
    "        '''\n",
    "        feats = []\n",
    "        for c in range(self.n_classes):\n",
    "            where = np.where(neighs_y == c)[0]\n",
    "            \n",
    "            if len(where) == 0:\n",
    "                feats += [neighs_dist[-1] + 1]\n",
    "            else:\n",
    "                feats += [neighs_dist[where[0]]]\n",
    "        \n",
    "        assert len(feats) == self.n_classes\n",
    "        return_list += [feats]\n",
    "        \n",
    "        '''\n",
    "            4. Minimum *normalized* distance to objects of each class\n",
    "               As 3. but we normalize (divide) the distances\n",
    "               by the distance to the closest neighbor.\n",
    "               \n",
    "               Do not forget to add self.eps to denominator\n",
    "        '''\n",
    "        feats = []\n",
    "        for c in range(self.n_classes):\n",
    "            where = np.where(neighs_y == c)[0]\n",
    "            dist_to_closest = neighs_dist[0] + 1e-6\n",
    "            \n",
    "            if len(where) == 0:\n",
    "                feats += [(neighs_dist[-1] + 1)/dist_to_closest]\n",
    "            else:\n",
    "                feats += [neighs_dist[where[0]]/dist_to_closest]\n",
    "        \n",
    "        assert len(feats) == self.n_classes\n",
    "        return_list += [feats]\n",
    "        \n",
    "        '''\n",
    "            5. \n",
    "               5.1 Distance to Kth neighbor\n",
    "                   Think of this as of quantiles of a distribution\n",
    "               5.2 Distance to Kth neighbor normalized by \n",
    "                   distance to the first neighbor\n",
    "               \n",
    "               feat_51, feat_52 are answers to 5.1. and 5.2.\n",
    "               should be scalars\n",
    "        '''\n",
    "        for k in self.k_list:\n",
    "            feat_51 = neighs_dist[k-1]\n",
    "            feat_52 = neighs_dist[k-1]/(neighs_dist[0] + 1e-6)\n",
    "            \n",
    "            return_list += [[feat_51, feat_52]]\n",
    "        \n",
    "        '''\n",
    "            6. Mean distance to neighbors of each class for each K from `k_list` \n",
    "                   For each class select the neighbors of that class among K nearest neighbors \n",
    "                   and compute average distance to those objects\n",
    "                   \n",
    "                   If there are no objects of a certain class among K neighbors, set mean distance to 999\n",
    "                   \n",
    "               You can use `np.bincount` with appropriate weights\n",
    "               Don't forget, that if you divide by something, \n",
    "               You need to add `self.eps` to denominator\n",
    "        '''\n",
    "        for k in self.k_list:\n",
    "            \n",
    "            dist_sum = np.bincount(neighs_y[:k], weights=neighs_dist[:k], minlength=self.n_classes)\n",
    "            counts = np.bincount(neighs_y[:k], minlength=self.n_classes)\n",
    "            feats = dist_sum/(counts+self.eps)\n",
    "            \n",
    "            feats[counts==0] = 999\n",
    "            \n",
    "            assert len(feats) == self.n_classes\n",
    "            return_list += [feats]\n",
    "        \n",
    "        \n",
    "        # merge\n",
    "        knn_feats = np.hstack(return_list)\n",
    "        \n",
    "        assert knn_feats.shape == (239,) or knn_feats.shape == (239, 1)\n",
    "        return knn_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you've implemented everything correctly we provide you the correct features for the first 50 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "CPU times: user 680 ms, sys: 0 ns, total: 680 ms\n",
      "Wall time: 786 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# a list of K in KNN \n",
    "k_list = [3, 8,32]\n",
    "\n",
    "# Load correct features\n",
    "true_knn_feats_first50 = np.load('../readonly/KNN_features_data/knn_feats_test_first50.npy')\n",
    "\n",
    "# Create instance of our KNN feature extractor\n",
    "NNF = NearestNeighborsFeats(n_jobs=1, k_list=k_list, metric='minkowski')\n",
    "\n",
    "# Fit on train set\n",
    "NNF.fit(X,Y)\n",
    "\n",
    "# Get features for test\n",
    "test_knn_feats = NNF.predict(X_test[:50])\n",
    "\n",
    "# This should be zero\n",
    "print (np.abs(test_knn_feats - true_knn_feats_first50).sum())\n",
    "# print (np.abs(test_knn_feats - test_knn_feats1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  32.,  32.,   0.,  24.,  31.,  16.,  30.,   0.,  20.,  21.,\n",
       "        32.,   0.,  31.,  20.,   0.,  18.,  32.,   0.,   0.,  31.,   0.,\n",
       "        32.,   0.,  17.,   0.,  26.,  32.,   0.,  32.,  31.,  27.,  14.,\n",
       "        17.,   4.,  30.,  26.,  25.,  32.,  25.,   0.,  25.,   0.,   0.,\n",
       "        31.,  30.,  27.,  27.,  23.,  31.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_knn_feats - true_knn_feats_first50).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_knn_feats[1][87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_knn_feats1 = test_knn_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CPU times: user 9.06 s, sys: 4 ms, total: 9.07 s\n",
    "Wall time: 10.5 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement parallel computations and compute features for the train and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute features for the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minkowski\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Not implemented, use n_jobs=1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-47d3c1301ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get features for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtest_knn_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Dump the features to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-6f28ae47ae67>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# http://qingkaikong.blogspot.ru/2016/12/python-parallel-method-in-class.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Not implemented, use n_jobs=1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# YOUR CODE GOES HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Not implemented, use n_jobs=1"
     ]
    }
   ],
   "source": [
    "for m in ['minkowski', 'cosine']:\n",
    "    print (m)\n",
    "    \n",
    "    # Create instance of our KNN feature extractor\n",
    "    NNF = NearestNeighborsFeats(n_jobs=20, k_list=k_list, metric=m)\n",
    "    \n",
    "    # Fit on train set\n",
    "    NNF.fit(X,Y)\n",
    "\n",
    "    # Get features for test\n",
    "    test_knn_feats = NNF.predict(X_test)\n",
    "    \n",
    "    # Dump the features to disk\n",
    "    np.save('data/knn_feats_%s_test.npy' % m , test_knn_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features for train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features for train, using out-of-fold strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Differently from other homework we will not implement OOF predictions ourselves\n",
    "# but use sklearn's `cross_val_predict`\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# a list of K in KNN, starts with 1\n",
    "k_list = [3, 8,32]\n",
    "\n",
    "# We will use two metrics for KNN\n",
    "for m in ['minkowski', 'cosine']:\n",
    "    print m\n",
    "    \n",
    "    # Set up splitting scheme, use StratifiedKFold\n",
    "    # use skf_seed and n_splits defined above\n",
    "    skf = # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Create instance of our KNN feature extractor\n",
    "    # n_jobs can be larger than the number of cores\n",
    "    NNF = NearestNeighborsFeats(n_jobs=10, k_list=k_list, metric=m)\n",
    "    \n",
    "    # Get KNN features using OOF use cross_val_predict with right parameters\n",
    "    preds = # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Save the features\n",
    "    np.save('data/knn_feats_%s_train.npy' % m, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made the above cells work, just run the following cell to produce a file to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/knn_feats_minkowski_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c8401370bb2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'minkowski'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cosine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mknn_feats_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/knn_feats_%s_train.npy'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mknn_feats_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/knn_feats_%s_test.npy'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/knn_feats_minkowski_train.npy'"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for m in ['minkowski', 'cosine']:\n",
    "    knn_feats_train = np.load('data/knn_feats_%s_train.npy' % m)\n",
    "    knn_feats_test = np.load('data/knn_feats_%s_test.npy' % m)\n",
    "    \n",
    "    s += knn_feats_train.mean() + knn_feats_test.mean()\n",
    "    \n",
    "print (s)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
